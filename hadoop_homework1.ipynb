{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f70eb368-85d7-4d11-8792-7450fe6973ba",
   "metadata": {},
   "source": [
    "Nantawat Sukrisunt 6510545543\n",
    "Naytitorn Chaovirachot 6510545560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cc7a6a5-24a8-4b1f-9f7c-e6dd91f7e79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16371 SecondaryNameNode\n",
      "16003 NameNode\n",
      "16170 DataNode\n",
      "16555 ResourceManager\n",
      "16699 NodeManager\n",
      "163021 Jps\n"
     ]
    }
   ],
   "source": [
    "!jps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "652ec7f8-e9e6-4722-bf07-10d4db72dfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source ~/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82ad2589-e095-4f80-ad62-540d5543f7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
      " or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]\n",
      "  where CLASSNAME is a user-provided Java class\n",
      "\n",
      "  OPTIONS is none or any of:\n",
      "\n",
      "buildpaths                       attempt to add class files from build tree\n",
      "--config dir                     Hadoop config directory\n",
      "--debug                          turn on shell script debug mode\n",
      "--help                           usage information\n",
      "hostnames list[,of,host,names]   hosts to use in slave mode\n",
      "hosts filename                   list of hosts to use in slave mode\n",
      "loglevel level                   set the log4j level for this command\n",
      "workers                          turn on worker mode\n",
      "\n",
      "  SUBCOMMAND is one of:\n",
      "\n",
      "\n",
      "    Admin Commands:\n",
      "\n",
      "daemonlog     get/set the log level for each daemon\n",
      "\n",
      "    Client Commands:\n",
      "\n",
      "archive       create a Hadoop archive\n",
      "checknative   check native Hadoop and compression libraries availability\n",
      "classpath     prints the class path needed to get the Hadoop jar and the\n",
      "              required libraries\n",
      "conftest      validate configuration XML files\n",
      "credential    interact with credential providers\n",
      "distch        distributed metadata changer\n",
      "distcp        copy file or directories recursively\n",
      "dtutil        operations related to delegation tokens\n",
      "envvars       display computed Hadoop environment variables\n",
      "fs            run a generic filesystem user client\n",
      "gridmix       submit a mix of synthetic job, modeling a profiled from\n",
      "              production load\n",
      "jar <jar>     run a jar file. NOTE: please use \"yarn jar\" to launch YARN\n",
      "              applications, not this command.\n",
      "jnipath       prints the java.library.path\n",
      "kdiag         Diagnose Kerberos Problems\n",
      "kerbname      show auth_to_local principal conversion\n",
      "key           manage keys via the KeyProvider\n",
      "rumenfolder   scale a rumen input trace\n",
      "rumentrace    convert logs into a rumen trace\n",
      "s3guard       manage metadata on S3\n",
      "trace         view and modify Hadoop tracing settings\n",
      "version       print the version\n",
      "\n",
      "    Daemon Commands:\n",
      "\n",
      "kms           run KMS, the Key Management Server\n",
      "\n",
      "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
     ]
    }
   ],
   "source": [
    "!~/hadoop/bin/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afe2bde9-c3a6-4e64-92a9-3d76f7bd8dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Use of this script to execute dfs is deprecated.\n",
      "WARNING: Attempting to execute replacement \"hdfs dfs\" instead.\n",
      "\n",
      "Found 5 items\n",
      "-rw-r--r--   1 hadoop supergroup      85834 2025-01-20 15:17 /27045.txt\n",
      "drwxr-xr-x   - hadoop supergroup          0 2025-02-04 20:38 /gutenberg\n",
      "drwx------   - hadoop supergroup          0 2025-01-20 15:21 /tmp\n",
      "-rw-r--r--   1 hadoop supergroup 1708674492 2025-02-03 15:32 /yellow_tripdata_2016-01.csv\n",
      "-rw-r--r--   1 hadoop supergroup  431701740 2025-02-03 14:12 /yellow_tripdata_2016-01.csv.gz\n"
     ]
    }
   ],
   "source": [
    "# List files in /\n",
    "!~/hadoop/bin/hadoop dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bab0437d-ce72-4d7f-9f89-ee84347a5302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Use of this script to execute dfs is deprecated.\n",
      "WARNING: Attempting to execute replacement \"hdfs dfs\" instead.\n",
      "\n",
      "Deleted /gutenberg\n"
     ]
    }
   ],
   "source": [
    "# Delete /gutenberg directory\n",
    "!~/hadoop/bin/hadoop dfs -rm -r -f  /gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8f5590-0bbe-4a60-9160-5dc14d3b08cb",
   "metadata": {},
   "source": [
    "Test your hadoop wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36fc5a11-f633-4b9e-a2d3-8a009b31cb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-04 20:40:38,616 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [./mapper.py, ./reducer.py, /tmp/hadoop-unjar1280134622742346177/] [] /tmp/streamjob4516436508246949020.jar tmpDir=null\n",
      "2025-02-04 20:40:39,456 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2025-02-04 20:40:39,635 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2025-02-04 20:40:39,849 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1736778250975_0031\n",
      "2025-02-04 20:40:39,952 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2025-02-04 20:40:40,040 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2025-02-04 20:40:40,061 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2025-02-04 20:40:40,518 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-02-04 20:40:40,548 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2025-02-04 20:40:40,967 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2025-02-04 20:40:40,976 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2025-02-04 20:40:41,085 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2025-02-04 20:40:41,105 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1736778250975_0031\n",
      "2025-02-04 20:40:41,105 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-02-04 20:40:41,278 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-02-04 20:40:41,279 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-02-04 20:40:41,352 INFO impl.YarnClientImpl: Submitted application application_1736778250975_0031\n",
      "2025-02-04 20:40:41,385 INFO mapreduce.Job: The url to track the job: http://bigdata2024:8088/proxy/application_1736778250975_0031/\n",
      "2025-02-04 20:40:41,387 INFO mapreduce.Job: Running job: job_1736778250975_0031\n",
      "2025-02-04 20:40:47,477 INFO mapreduce.Job: Job job_1736778250975_0031 running in uber mode : false\n",
      "2025-02-04 20:40:47,479 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-02-04 20:40:51,531 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2025-02-04 20:40:52,546 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-02-04 20:40:56,566 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-02-04 20:40:57,578 INFO mapreduce.Job: Job job_1736778250975_0031 completed successfully\n",
      "2025-02-04 20:40:57,667 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=137805\n",
      "\t\tFILE: Number of bytes written=964235\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=90096\n",
      "\t\tHDFS: Number of bytes written=35474\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5224\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2538\n",
      "\t\tTotal time spent by all map tasks (ms)=5224\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2538\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5224\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2538\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=5349376\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2598912\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1695\n",
      "\t\tMap output records=13575\n",
      "\t\tMap output bytes=110649\n",
      "\t\tMap output materialized bytes=137811\n",
      "\t\tInput split bytes=166\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3587\n",
      "\t\tReduce shuffle bytes=137811\n",
      "\t\tReduce input records=13575\n",
      "\t\tReduce output records=3587\n",
      "\t\tSpilled Records=27150\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=208\n",
      "\t\tCPU time spent (ms)=2340\n",
      "\t\tPhysical memory (bytes) snapshot=868802560\n",
      "\t\tVirtual memory (bytes) snapshot=7635333120\n",
      "\t\tTotal committed heap usage (bytes)=771751936\n",
      "\t\tPeak Map Physical memory (bytes)=321966080\n",
      "\t\tPeak Map Virtual memory (bytes)=2544193536\n",
      "\t\tPeak Reduce Physical memory (bytes)=226754560\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2548183040\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=89930\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=35474\n",
      "2025-02-04 20:40:57,667 INFO streaming.StreamJob: Output directory: /gutenberg\n"
     ]
    }
   ],
   "source": [
    "!~/hadoop/bin/hadoop jar /home/hadoop/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar -file ./mapper.py    -mapper ./mapper.py -file ./reducer.py   -reducer ./reducer.py -input /27045.txt -output /gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3976b551-8ede-4408-8c7e-d0503c5ae001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Use of this script to execute dfs is deprecated.\n",
      "WARNING: Attempting to execute replacement \"hdfs dfs\" instead.\n",
      "\n",
      "put: `/yellow_tripdata_2016-01.csv': File exists\n"
     ]
    }
   ],
   "source": [
    "#command to bring input file to hdfs\n",
    "!~/hadoop/bin/hadoop dfs -put yellow_tripdata_2016-01.csv /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e386d171-495f-4347-a7b7-e6911519f237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper_yellow_trip.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_yellow_trip.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"mapper_yellow_trip.py\"\"\"\n",
    "\n",
    "import sys\n",
    "import gzip\n",
    "import datetime\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    fields = line.strip().split(',')  # Split the line by commas (CSV format)\n",
    "\n",
    "    try:\n",
    "        pickup_datetime = fields[1]  \n",
    "        trip_distance = float(fields[4])  \n",
    "        year = datetime.datetime.strptime(pickup_datetime, \"%Y-%m-%d %H:%M:%S\").year\n",
    "        if year == 2016:\n",
    "            print ('%s\\t%s' % (trip_distance, 1))\n",
    "\n",
    "    except ValueError as e:\n",
    "        continue  # Skip lines with invalid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e84e5ce5-8fec-4dc6-bcc2-38df49a4bb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer_yellow_trip.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_yellow_trip.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"reducer.py\"\"\"\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "total_distance = 0.0\n",
    "total_trips = 0\n",
    "\n",
    "# Input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    distance, count = line.strip().split(\"\\t\")\n",
    "    total_distance += float(distance)\n",
    "    total_trips += int(count)\n",
    "\n",
    "# Calculate and print the average distance if there are any trips\n",
    "if total_trips > 0:\n",
    "    print(f\"Average Distance in 2016: {total_distance / total_trips}\")\n",
    "else:\n",
    "    print(\"No trips in 2016\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8df30eb-4a11-4b30-a4b3-0c1a07fd9e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-04 20:41:00,389 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [./mapper_yellow_trip.py, ./reducer_yellow_trip.py, /tmp/hadoop-unjar8500934972552678528/] [] /tmp/streamjob2458273501056910097.jar tmpDir=null\n",
      "2025-02-04 20:41:01,250 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2025-02-04 20:41:01,426 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2025-02-04 20:41:01,646 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1736778250975_0032\n",
      "2025-02-04 20:41:01,751 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2025-02-04 20:41:01,831 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2025-02-04 20:41:02,251 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2025-02-04 20:41:02,715 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-02-04 20:41:02,745 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2025-02-04 20:41:02,767 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2025-02-04 20:41:02,778 INFO mapreduce.JobSubmitter: number of splits:13\n",
      "2025-02-04 20:41:02,911 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2025-02-04 20:41:02,926 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1736778250975_0032\n",
      "2025-02-04 20:41:02,927 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-02-04 20:41:03,114 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-02-04 20:41:03,114 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-02-04 20:41:03,186 INFO impl.YarnClientImpl: Submitted application application_1736778250975_0032\n",
      "2025-02-04 20:41:03,226 INFO mapreduce.Job: The url to track the job: http://bigdata2024:8088/proxy/application_1736778250975_0032/\n",
      "2025-02-04 20:41:03,228 INFO mapreduce.Job: Running job: job_1736778250975_0032\n",
      "2025-02-04 20:41:09,301 INFO mapreduce.Job: Job job_1736778250975_0032 running in uber mode : false\n",
      "2025-02-04 20:41:09,303 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-02-04 20:41:28,451 INFO mapreduce.Job:  map 11% reduce 0%\n",
      "2025-02-04 20:41:29,462 INFO mapreduce.Job:  map 18% reduce 0%\n",
      "2025-02-04 20:41:34,497 INFO mapreduce.Job:  map 24% reduce 0%\n",
      "2025-02-04 20:41:35,502 INFO mapreduce.Job:  map 28% reduce 0%\n",
      "2025-02-04 20:41:36,519 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2025-02-04 20:41:37,539 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2025-02-04 20:41:38,546 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "2025-02-04 20:41:39,550 INFO mapreduce.Job:  map 46% reduce 0%\n",
      "2025-02-04 20:41:54,622 INFO mapreduce.Job:  map 49% reduce 0%\n",
      "2025-02-04 20:41:56,637 INFO mapreduce.Job:  map 61% reduce 0%\n",
      "2025-02-04 20:41:57,642 INFO mapreduce.Job:  map 65% reduce 0%\n",
      "2025-02-04 20:42:00,661 INFO mapreduce.Job:  map 66% reduce 0%\n",
      "2025-02-04 20:42:02,671 INFO mapreduce.Job:  map 73% reduce 0%\n",
      "2025-02-04 20:42:03,677 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "2025-02-04 20:42:04,683 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "2025-02-04 20:42:05,688 INFO mapreduce.Job:  map 92% reduce 0%\n",
      "2025-02-04 20:42:16,740 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-02-04 20:42:19,753 INFO mapreduce.Job:  map 100% reduce 54%\n",
      "2025-02-04 20:42:25,781 INFO mapreduce.Job:  map 100% reduce 85%\n",
      "2025-02-04 20:42:29,807 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-02-04 20:42:30,817 INFO mapreduce.Job: Job job_1736778250975_0032 completed successfully\n",
      "2025-02-04 20:42:30,909 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=93037083\n",
      "\t\tFILE: Number of bytes written=189289772\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1708724957\n",
      "\t\tHDFS: Number of bytes written=42\n",
      "\t\tHDFS: Number of read operations=44\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=13\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=13\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=326706\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=23943\n",
      "\t\tTotal time spent by all map tasks (ms)=326706\n",
      "\t\tTotal time spent by all reduce tasks (ms)=23943\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=326706\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=23943\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=334546944\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=24517632\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10906859\n",
      "\t\tMap output records=10906858\n",
      "\t\tMap output bytes=71223361\n",
      "\t\tMap output materialized bytes=93037155\n",
      "\t\tInput split bytes=1313\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4513\n",
      "\t\tReduce shuffle bytes=93037155\n",
      "\t\tReduce input records=10906858\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=21813716\n",
      "\t\tShuffled Maps =13\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=13\n",
      "\t\tGC time elapsed (ms)=2091\n",
      "\t\tCPU time spent (ms)=166810\n",
      "\t\tPhysical memory (bytes) snapshot=4540051456\n",
      "\t\tVirtual memory (bytes) snapshot=35622817792\n",
      "\t\tTotal committed heap usage (bytes)=3987210240\n",
      "\t\tPeak Map Physical memory (bytes)=356171776\n",
      "\t\tPeak Map Virtual memory (bytes)=2562928640\n",
      "\t\tPeak Reduce Physical memory (bytes)=325750784\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2564644864\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1708723644\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=42\n",
      "2025-02-04 20:42:30,909 INFO streaming.StreamJob: Output directory: /yellow_trip_output\n"
     ]
    }
   ],
   "source": [
    "#command to run your program\n",
    "!~/hadoop/bin/hadoop jar /home/hadoop/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar -file ./mapper_yellow_trip.py    -mapper ./mapper_yellow_trip.py -file ./reducer_yellow_trip.py   -reducer ./reducer_yellow_trip.py -input /yellow_tripdata_2016-01.csv -output /yellow_trip_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd6e1d4c-1fda-4844-8b4f-07efd82e5bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Use of this script to execute dfs is deprecated.\n",
      "WARNING: Attempting to execute replacement \"hdfs dfs\" instead.\n",
      "\n",
      "2025-02-04 20:51:09,826 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "Average Distance in 2016: 4.648196988544\t\n"
     ]
    }
   ],
   "source": [
    "#command to show output\n",
    "!~/hadoop/bin/hadoop dfs -cat /yellow_trip_output/part-00000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
